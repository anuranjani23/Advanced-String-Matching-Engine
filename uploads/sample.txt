Personalized movie recommendation in IoT-enhanced systems using graph convolutional network and multi-layer perceptron | Scientific Reports Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Personalized movie recommendation in IoT-enhanced systems using graph convolutional network and multi-layer perceptron Download PDF Download PDF Subjects Computational scienceComputer scienceScientific data AbstractExploring the optimization of communication strategies for animation films in the context of cross-cultural communication, this research integrates the Internet of Things (IoT) and convolutional networks. The research constructs a collaborative filtering (CF) movie recommendation model based on a graph convolutional neural network (GCN) and investigates its application in cross-cultural communication. The fusion of IoT and convolutional networks in movie communication is also analyzed, and the effectiveness of the proposed GCN-CF model is validated through comparative experiments. The results indicate that, compared to other models, the GCN-CF model achieves the lowest Root Mean Square Error (RMSE) on the MovieLens 100 K and MovieLens 1 M datasets, with values of 0.8762 and 0.8275, respectively. Compared to traditional models, the GCN-CF model exhibits significantly superior performance in terms of RMSE, with reductions ranging from 0.6 to 5.2%, highlighting its heightened detection accuracy and overall performance. Moreover, the performance of the GCN-CF model is enhanced after introducing attention mechanisms and auxiliary information on both datasets, showing an improvement of 0.4% compared to the scenario without these additions. This data demonstrates the effectiveness of attention mechanisms and auxiliary information. Finally, the research presents an animation film communication strategy based on IoT and convolutional networks, offering novel insights for film production and communication, along with positive implications for cultural exchange and the advancement of the global media industry. Similar content being viewed by others A data-driven short video international communication model based on indicator system communication network and attention BiLSTM neural network Article Open access 24 June 2024 Amharic political sentiment analysis using deep learning approaches Article Open access 20 October 2023 Graph neural network recommendation algorithm based on improved dual tower model Article Open access 15 February 2024 IntroductionResearch background and motivationsIn the contemporary digital epoch, animation films, functioning as a fundamental medium, assume an indisputable role in the realm of cultural communication. Given the ubiquity of Internet technology and the pervasive influence of social media, animation films have transcended geographical confines, experiencing expeditious global dissemination1. Nevertheless, audiences emanating from diverse cultural milieus manifest distinct preferences, emotional responses, and interpretative frameworks with respect to animation films, thereby engendering novel challenges and opportunities in the domain of cross-cultural animation film communication. Furthermore, the escalating demands of personalized audience preferences pose a formidable challenge to conventional communication paradigms, necessitating innovative approaches to accommodate this inherent diversity2. Consequently, this research endeavors to scrutinize inventive and efficacious communication strategies to facilitate the successful propagation of animation films across disparate cultural contexts.The advent of the Internet of Things (IoT) technology has led to unprecedented possibilities for film communication. The amalgamation of smart devices, sensors, and network connectivity within the IoT framework facilitates real-time interaction between audiences and films. Simultaneously, convolutional neural networks (CNNs), as an advanced facet of deep learning technology, have attained notable strides in the domains of image processing and feature extraction3. In the sphere of movie recommendations, CNNs proficiently distill pertinent features from diverse visual elements such as movie posters and stills, thereby furnishing more comprehensive and accurate data for recommendation systems.However, despite the impressive performance of CNNs in processing image data, their capability in handling non-Euclidean space data, such as graph-structured data, is limited. In recommendation systems, especially when facing complex user-item interaction relationships, graph convolutional neural network (GCN) demonstrate their unique advantages. GCN effectively processes graph-structured data by performing convolution operations on the neighbors of nodes, allowing them to capture the intricate relationships and structural information within graph data. This makes GCN particularly outstanding in recommendation systems, especially in addressing the relationships between users and items.To overcome the limitations of traditional methods, the GCN-Collaborative Filtering (GCN-CF) model has emerged. The GCN-CF model combines GCN with CF techniques to propose a new recommendation approach. This model utilizes graph convolution operations to model the relationships between users and items while incorporating the principles of CF, thus better handling sparse data and capturing users’ personalized needs. By integrating the powerful modeling capabilities of graph-structured data with traditional recommendation techniques, the GCN-CF model showcases significant innovation in enhancing the overall performance of recommendation systems.Against this backdrop, the principal objective of this research is to delve into the optimization of strategies for communication pertaining to animation films through the integration of IoT and convolutional networks. The research aims to integrate graph convolutional neural network (GCN) with CF approaches, presenting an innovative recommendation model known as GCN-CF. This model extensively harnesses CF information inherent in user historical data and augments user modeling by incorporating additional user and item-related information. The overarching aspiration of this research is to contribute innovative insights and methodological frameworks for the optimization of cross-cultural animation film communication strategies, thereby engendering broader and more profound impacts.The innovation of this research lies in the introduction of a dynamic attention mechanism to improve the traditional GCN-CF model, particularly addressing the challenges of cross-cultural recommendations. The dynamic attention mechanism can adjust weights in real-time based on user interactions with movies, providing more personalized and precise recommendations. Unlike traditional static or predefined weight attention mechanisms, the proposed method employs a multi-level attention structure that balances user behavior and cultural background differences from various levels, thereby capturing users’ fine-grained preferences. This mechanism specifically considers users’ cultural backgrounds, allowing the model to dynamically adjust recommendation strategies when encountering users from diverse cultural contexts, enhancing the relevance and cultural adaptability of recommendations. Additionally, the dynamic attention mechanism possesses adaptive learning capabilities, enabling it to automatically adjust parameters based on changes in user behavior and real-time feedback, maintaining the system’s efficiency and accuracy. This innovation provides a unique solution to the complexities involved in cross-cultural recommendations, significantly improving the model’s performance. Through these enhancements, this research not only expands the application scope of the existing GCN-CF algorithm but also introduces new methodologies in the recommendation system domain, promoting the advancement of cross-cultural recommendation technologies.Research objectivesThe principal objectives of this research encompass the introduction of a GCN-CF recommendation model. The aim is to comprehensively leverage CF information extracted from user historical data, thereby augmenting the precision and personalization capabilities inherent in traditional recommendation methodologies. The integration of smart devices and sensors within this research framework serves to facilitate real-time interaction between the audience and the cinematic narrative, thereby introducing a novel dimension to the landscape of film distribution. The incorporation of IoT technology into communication strategies is anticipated to enhance audience engagement and elevate the interactive elements embedded within cinematic experiences. Specifically, the primary objective of this research is to explore how the fusion of the IoT and convolutional networks can optimize the dissemination strategy of animated films within the context of cross-cultural communication. Grounded in GCN, the research establishes a GCN-CF movie recommendation model, denoted as GCN-CF and validates its effectiveness in the realm of cross-cultural communication through empirical experiments.Through the pursuit of these research objectives, this investigation endeavors to furnish novel perspectives and methodological approaches for the optimization of communication strategies related to animation films. The overarching objective is to contribute to the broadened recognition and resonance of cross-cultural films on a global scale. By innovating the recommendation system in the context of cross-cultural communication, improving model performance, and proposing dissemination strategies, this research provides valuable theoretical and practical insights into technological advancement and cultural dissemination within the film and media industry. The research, with its focus on strengthening cultural exchange and driving the global media industry forward, holds positive and far-reaching implications.Literature reviewThe transference of information across diverse cultural contexts, commonly referred to as cross-cultural communication, has become a focal point of extensive consideration in recent years. The expeditious development of the IoT has presented innovative prospects within the communication domain. The interactive and personalized attributes inherent to IoT have introduced novel perspectives for the evolution of communication media. Nauman et al. (2020) conducted a survey elucidating various multimedia applications supported by IoT, exemplifying diverse applications in areas such as road traffic management, safety, industry, and health. Their work showcased the transformative impact of these applications on human lives4. Chaudhuri et al. (2022), employing the paradigm of cultural repositioning, inclusive of cultural adaptation and integration, investigated the influence of assimilation, integration, and isolation on knowledge dissemination through Internet and word-of-mouth communication. Their findings indicated that IoT contributes to the enhancement of traditional word-of-mouth communication processes and plays a role in shaping immigrants’ cultural intentions and knowledge propagation5. In the realm of Shaanxi culture’s shadow play images, Xie and Yin (2022) proposed a Convolutional Block Attention Module (CBAM)-ResNet50-based shadow play image retrieval model, integrated within an IoT system for more effective deep cultural information retrieval6. The application of IoT additionally empowers filmmakers to gain profound insights into audience feedback and preferences, thereby optimizing recommendation and communication strategies.Deep learning methodologies have attained notable milestones in the field of image processing, prompting researchers to extend their application, including the integration of CNNs into recommendation systems to augment their efficiency. Zhang et al. (2021) explored the utilization of artificial intelligence (AI) in recommendation systems, examining fundamental methods and mainstream technologies within these systems. Their work delved into how AI effectively contributes to the technological development and application of recommendation systems, reviewing advancements facilitated by AI methodologies such as fuzzy techniques, transfer learning, genetic algorithms, evolutionary algorithms, neural networks, deep learning, and active learning7. Addressing the issue of information overload, particularly in domains such as e-commerce, entertainment, and social media, Da’u and Salim (2021) emphasized the significance of recommendation systems. They provided insights for researchers and practitioners to comprehend emerging trends and challenges in this field8. Singh (2022) developed a neural network-based music recommendation algorithm, demonstrating its efficacy across six song categories through the automatic suggestion of music based on user preferences9. Özçelik and Altan (2023) introduced an AI-based model that, by processing and extracting features from fundus images, successfully overcame the nonlinear dynamics in diabetic retinopathy classification with low computational complexity, demonstrating efficiency and accuracy in diagnosing various stages of diabetic retinopathy10. Karasu and Altan (2022) conducted a classification research using region-based fundus establishment and multi-layer perceptrons (MLP) in deep learning to differentiate between crops and weeds. The results indicated the high performance of this model11. Yağ and Altan (2022) proposed a novel model for plant leaf disease classification. They achieved high accuracy and low computational complexity by employing a wrapper method incorporating the pollen dissemination algorithm and support vector machines, along with the CNNs classifier, and utilizing a wrapper feature selection method based on metaheuristic optimization techniques. The model successfully realized real-time leaf disease classification in apple, grape, and tomato plants12.In conclusion, neural networks, including CNNs, can extract features from diverse data sources, encompassing movie posters and review texts, to comprehensively capture visual and semantic information embedded in films.Nevertheless, despite these advancements, further exploration remains requisite, particularly in the specific context of cross-cultural animation film communication. While the aforementioned studies highlight the significance of cross-cultural communication, they may not fully address the cross-cultural context within the specific domain of animated films. Animated films often embody unique cultural elements and aesthetic standards, necessitating more in-depth research to comprehend the diverse reactions of audiences from different cultures towards animated films. The neural networks and deep learning algorithms mentioned in these studies predominantly focus on the realm of recommendation systems. However, for other aspects of animated film dissemination, such as social interaction and advertising communication, there may be a need for diverse types of algorithms and methods. Moreover, there is a lack of in-depth exploration into media interaction and social media in the mentioned studies. This research endeavors to delve into the integration of these technologies to optimize the dissemination effectiveness of cross-cultural animation films.Research methodologyCF recommendation model based on GCNThe fundamental objective of recommendation systems is to infer the extent of a user’s preference for a specific item by leveraging their historical interaction data, encompassing activities such as purchases and clicks13. CF, a classical recommendation algorithm, is employed to predict items or content that users may find appealing, drawing insights from both their historical behaviors and the behavioral patterns exhibited by other users14. Within existing recommendation models, the Graph Convolutional Matrix Completion (GC-MC) model is characterized by the utilization of a solitary layer of graph convolution to explore a user’s historical interaction data, thereby falling short in capturing high-order connectivity information15. Conversely, the Graph Sample and Aggregated (GraphSA) model exclusively considers the collaborative impact between items, overlooking the collaborative dynamics between items and users16.In order to overcome the limitations inherent in current models, this research posits the introduction of a GCN-CF. This proposed approach systematically delves into the high-order connectivity information embedded within the graph, extracting CF insights inherent in a user’s historical data. Additionally, an attention mechanism is incorporated to aggregate information from diverse neighboring nodes, assigning varying weights to enhance the discriminative capacity of the model. Furthermore, supplementary information concerning users and items is integrated to model user interests more precisely, thereby generating more nuanced latent feature vectors for both users and items. The holistic model structure is delineated in Fig. 117. The comprehensive GCN-CF model consists of a representation learning component based on GCN and a combination learning component grounded in the MLP. For the representation learning segment based on graph CNNs, this research adopts a three-layer graph convolutional structure while incorporating an attention mechanism. This is aimed at capturing high-order connectivity information in user’s historical interaction data to generate more accurate vector representations. In the experiment, the combination learning segment relies on the MLP. Through the fusion and analysis by the MLP, various feature domains of user and item auxiliary information are integrated, thoroughly exploring the combination relationships among multiple feature domains and obtaining vector representations for user and item auxiliary information.Figure 1Structure of the CF recommendation model based on GCN.Full size imageThe representation learning component, founded on GCN, establishes a network for message propagation within the user-item interaction graph. Through a comprehensive exploration of the graph structure information inherent in the user-item interaction network, this component adeptly captures CF insights embedded in the user’s historical interaction data. Consequently, it facilitates a more precise modeling of user interest preferences18. Random low-dimensional vectors \(\:\{{c}_{u},{c}_{i}\}\in\:{\mathbb{S}}^{d}\) are systematically generated for users and items utilizing their respective identifiers (IDs), thereby serving as the initial feature vectors. The optimization process is executed in an end-to-end manner, where d signifies the dimensionality of the vector space. For all users and items, the parameter matrix is denoted as Eq. (1):$$\:C=[{c}_{u1},\cdots\:,{c}_{uN},\:{c}_{i4},\cdots\:,{c}_{iM}]$$ (1) In Eq. (1), the variables M and N represent the quantities of items and users, respectively.The propagation of higher-order information extends from first-order information propagation, encompassing the incorporation of multiple graph convolution layers. This iterative process captures high-order connectivity information within the user interaction network, achieved through the stacking of successive graph convolution layers. The implementation of this approach maximizes the utilization of CF information embedded in the user’s historical interaction data, as depicted in Fig. 219.Figure 2Illustration of higher-order information propagation.Full size imageIn Fig. 2, through the stacking of l layers of graph convolutional layers, a node becomes receptive to information propagation from its l-th order neighboring nodes. The vector representation of node u in the l-th layer is delineated by Eq. (2):$$\:{c}_{u}^{l}=LeakyReLU({m}_{u\leftarrow\:u}^{l}+\sum\:_{i\in\:{N}_{u}}{m}_{u\leftarrow\:i}^{l})$$ (2) In Eq. (2), the terms are defined as follows: \(\:{m}_{u\leftarrow\:u}^{l}\) represents the self-loop information propagation and \(\:{m}_{u\leftarrow\:i}^{l}\) represents the propagation of information from neighboring nodes. The precise formulations for these terms are provided in Eqs. (3) and (4):$$\:{m}_{u\leftarrow\:u}^{l}={\alpha\:}_{ui}\left({Q}_{1}^{l}{c}_{i}^{l-1}\right)$$ (3) $$\:{m}_{u\leftarrow\:i}^{l}={Q}_{1}^{l}{c}_{u}^{l-1}$$ (4) Equations (3) and (4) are defined as follows: where \(\:{\alpha\:}_{ui}\) serves as a coefficient for adjusting the weight of neighbor node information in the self-loop information. The parameter matrix \(\:{Q}_{1}^{l}\in\:{\mathbb{S}}^{{d}_{l}*{d}_{l-1}}\) is employed for feature transformation, with \(\:{d}_{l}\) representing the feature dimension post-transformation. The feature vector \(\:{c}_{i}^{l-1}\) is generated for item i in the (l-1)-th layer. Subsequently, at the l-th layer, \(\:{c}_{i}^{l-1}\) undergoes further propagation to model the vector representation of node u in the l-th layer. Analogously, employing a similar procedural approach, the vector representation \(\:{c}_{i}^{l}\) for item node i in the l-th layer can be derived. The elucidation of the three-order information propagation process for user u1 is depicted in Fig. 320.Figure 3Three-order information propagation process for user u1.Full size imageIn Fig. 3, \(\:{c}_{u1}^{3}\) signifies the vector representation of user u1 subsequent to the integration of information from its third-order neighboring nodes. The resultant node vector representation, derived through successive layers of graph convolutional layers, encapsulates extensive CF information originating from the graph structure. Notably, this encompasses information pertaining to paths such as \(\:{i}_{4}\to\:{u}_{2}\to\:{i}_{2}\to\:{u}_{1}\), thereby providing a nuanced characterization of the attributes associated with both users and items21.The component of combination learning, predicated on the MLP, facilitates cross-interactions among diverse feature domains. This enables the model to discern relationships spanning disparate feature domains and, consequently, enhances its expressive capacity22. User features such as user IDs, age, gender, etc., are vectorized, and the resultant feature vectors are concatenated through the Concatenation (Concat) layer to yield the initial feature vector \(\:{c}_{u}^{f}\). This vector is subsequently input into the MLP to generate the feature representation \(\:{g}_{u}^{f}\) for user auxiliary information, as delineated in Eq. (5):$$\:{g}_{u}^{f}=\epsilon\:({Q}_{1}^{u,f}{c}_{u}^{f}+{b}^{u})$$ (5) In Eq. (5), \(\:{Q}_{1}^{u,f}\) represents the weight matrix. \(\:{b}^{u}\) is the bias vector. \(\:\epsilon\:\) is the activation function. Analogously, pertaining to items, features such as movie titles, genres, etc., undergo a vectorization process, as depicted in Eq. (6):$$\:{g}_{i}^{f}=\epsilon\:({Q}_{1}^{i,f}{c}_{i}^{f}+{b}^{i})$$ (6) In Eq. (6), \(\:{c}_{\text{i}}^{f}\) is the initial feature vector of the item (movie), and \(\:{g}_{\text{i}}^{f}\) is the feature representation of item auxiliary information.After processing through L layers of graph convolutional layers, aggregation is performed using vector concatenation to obtain the user’s vector representation \(\:{c}_{u}^{*}\). It is combined with \(\:{g}_{u}^{f}\) and input into the MLP layer to generate the final user vector representation \(\:{w}_{u}\):$$\:{w}_{u}=\epsilon\:({Q}_{2}^{u}{c}_{u}^{*}+{Q}_{2}^{u,f}{g}_{u}^{f})$$ (7) The final vector \(\:{w}_{i}\) for items is given by Eq. (8):$$\:{w}_{i}=\epsilon\:({Q}_{2}^{i}{c}_{u}^{*}+{Q}_{2}^{i,f}{g}_{i}^{f})$$ (8) The vector dot-product operation is used to predict the user’s rating for the item, denoted as \(\:{\widehat{y}}_{ui}\):$$\:{\widehat{y}}_{ui}={\left({w}_{u}\right)}^{T}{w}_{i}$$ (9) To further enhance innovation, the proposed method introduces a dynamic attention mechanism based on these existing technologies, allowing for a refined adjustment of the relationship between users and movies across different cultural backgrounds. Specifically, the core of this mechanism lies in its ability to dynamically adjust attention weights based on the interactions between users and movies, differing from traditional static or predefined weight attention mechanisms. By calculating and adjusting weights in real time during the recommendation process, the dynamic attention mechanism can more accurately reflect users’ current preferences and needs, thereby improving the relevance and accuracy of recommendations.The experiments utilize a multi-level attention structure, enabling the model to weigh the complex relationships between users and movies from different levels. Specifically, the model focuses on direct user-movie interactions at a lower level while considering user behavior patterns and cultural background differences at a higher level. This multi-level weighting mechanism helps the model capture users’ fine-grained preferences, thus enhancing the degree of personalization in recommendations. Moreover, the dynamic attention mechanism specifically accounts for cultural differences in cross-cultural scenarios. By integrating users’ cultural backgrounds or the cultural tags of movies into the attention mechanism, the model can better adapt to these cultural differences during recommendations, providing results that align with cultural contexts.Additionally, the dynamic attention mechanism possesses adaptive learning capabilities, allowing it to automatically adjust its parameters based on real-time feedback and changes in user behavior. This adaptability ensures that the model can quickly adjust to changes in user preferences or emerging cultural trends, maintaining the efficiency and accuracy of the recommendation system. Through these improvements, the dynamic attention mechanism not only enhances the model’s ability to meet diverse user needs but also provides an innovative solution to the complexities of cross-cultural recommendations, making the recommendation results more personalized and culturally sensitive.Fusion strategy of iot and convolutional networksThe psychological and emotional experiences of audiences originating from diverse cultural backgrounds exert a substantial influence on the dissemination of films23. Within the framework of cross-cultural communication theory, emphasis is placed on elucidating the process of information transmission and meaning construction between disparate cultures, thereby delving into the impact of cultural factors on the reception and interpretation of information. In the specific context of animated film dissemination, audiences representing varied cultural backgrounds may exhibit disparate emotional responses and interpretations of identical content, necessitating the formulation of tailored communication strategies24. The advent of the IoT introduces novel prospects for the propagation of films. When amalgamated with convolutional networks, it engenders heightened precision in recommendations and facilitates interactive viewing experiences. The integration of these two elements within the domain of film propagation is visually elucidated in Fig. 4.Figure 4Application of fusion strategy of iot and convolutional networks in film propagation.Full size imageIn Fig. 4, the fusion of IoT and convolutional networks manifests in several key aspects. Firstly, IoT facilitates real-time interaction between the audience and the film by leveraging smart devices and sensors. This engagement allows viewers to actively participate in the cinematic experience while the convolutional network-based recommendation system analyzes interactive behaviors and preferences to provide personalized recommendations. Secondly, IoT contributes to the enrichment of film content. By employing smart devices and sensors to scan specific images or objects within the film, viewers gain access to supplementary background information and behind-the-scenes content. These data sets deepen the comprehension of viewer preferences coupled with content analysis from convolutional networks. Lastly, IoT provides additional contextual information for CF. Analyzing interactive behaviors and emotional fluctuations during the viewing process enhances the predictive accuracy of preferences. Convolutional networks leverage this contextual information to refine recommendations25. Furthermore, in certain scenarios, blockchain technology may be pertinent, addressing issues related to copyright, digital assets, smart contracts, or payments in the context of movie distribution. Originally conceived as the underlying technology for the digital currency Bitcoin, blockchain is characterized by its decentralized distributed ledger structure, serving as a secure and transparent database for recording transactions and data. The technology’s attributes include decentralization, immutability, security, and transparency, rendering it applicable across various domains such as finance, supply chain, voting, healthcare, and copyright management. Its potential benefits encompass enhanced data security, reduced intermediary costs, improved transparency, and the facilitation of trust-based applications. The necessity of blockchain technology in the context of movie distribution and recommendations is contingent upon specific application scenarios.In summary, the integration of IoT and convolutional networks in film distribution yields a transformative strategy. Leveraging the IoT through smart devices and sensors enables audiences to engage in real-time interactions during movies. These sensors capture audience behavior and feedback. The IoT can collect data on audience behavior, such as preferences, interests, and interactions, through smart devices. IoT technology can also offer additional movie content. For example, by scanning specific images or objects, viewers can access background information, behind-the-scenes anecdotes, and more. GCN can exploit high-order connectivity information in the graph to comprehensively uncover CF information in user historical data, contributing to a better understanding of user interests. The GCN-based CF recommendation model can more accurately reveal relationships between users and items, generating more expressive latent feature vectors for users and items. IoT technology provides personalized movie recommendations through real-time interaction. These interactive behaviors, captured by IoT sensors, provide GCN with additional context information. The supplementary IoT data, such as audience feedback and behavior, combined with the user models learned by GCN, can enhance the understanding of audience preferences and improve recommendation accuracy. The fusion of GCN and IoT can provide more precise CF information, optimizing the recommendation model by considering additional dimensions of data, such as real-time interaction and supplementary content information. Through this integrated application, the fusion strategy of IoT and GCN aims to offer a more enriching movie-watching experience while providing movie producers with more targeted dissemination strategies, allowing films to better adapt to diverse cultures and audience needs.Hence, the integration of IoT sensors introduces a novel interactive dimension to the distribution of films, reinforcing the connection between the audience and the cinematic narrative, thereby elevating audience satisfaction and enriching the overall viewing experience. In practical scenarios, IoT sensors encompassing visual recognition, motion tracking, emotion recognition, environmental monitoring, and similar functionalities contribute significantly to enhancing the audience’s cinematic engagement. Viewers have the capability to utilize visual recognition sensors integrated into smartphones or glasses, enabling them to scan specific images or objects within the film for relevant information. For instance, in the context of an animated film, viewers may employ this technology to scan a character’s poster, accessing background information about said character. Motion sensors facilitate gesture control, enabling viewers to interact with movie characters through gestures or influence the direction of the plot. Certain emotion recognition sensors can discern the audience’s emotional states; facial expression recognition sensors, for instance, capture emotional reactions, facilitate adjustments to the film’s emotional elements, or offer more pertinent recommendations. Environmental sensors, attuned to factors such as temperature, light, and sound levels, contribute to a more adaptive viewing experience. For example, light sensors may adjust screen brightness to accommodate varying lighting conditions in a movie theater setting.The integration of the IoT, GCN, and CF can optimize cross-cultural communication in animated movies. Firstly, IoT technology provides audiences with real-time interaction and a more immersive viewing experience. Through smart devices and sensors, viewers can actively participate in the film, and this interactive data can be utilized for more personalized recommendations. Secondly, GCN, as a robust tool for graph-structured learning, can thoroughly explore CF information in user historical data, thereby enhancing the accuracy of the recommendation system. Lastly, CF, by analyzing user preferences and behavior patterns, offers personalized recommendations. The fusion strategy that combines IoT and GCN more accurately captures cultural differences and emotional experiences of audiences in a cross-cultural context, thereby optimizing the dissemination effectiveness of animated movies. This comprehensive utilization of IoT, GCN, and CF not only improves the performance of the recommendation system but also better meets the diverse cultural needs of audiences, providing an innovative and rational solution for cross-cultural communication.Code availabilityThe implementation of the algorithms and models used, including data preprocessing, model training, and evaluation, is provided in the appendix. All relevant code is fully documented to ensure the reproducibility of the experimental results and to allow readers to adapt and apply these models to other datasets or scenarios. The code details are as follows: (1) Dataset processing: The code loads the MovieLens 100 K or 1 M dataset and splits it into training and test sets. It also initializes user and movie embedding vectors for feature learning during model training. (2) GCN-CF model: The model consists of a three-layer GCN and a MLP. The GCN generates feature vectors for users and movies, while the MLP combines these feature vectors to predict user ratings. (3) Hyperparameter settings: In the code, hyperparameters, such as embedding dimensions, learning rate, and regularization coefficient, are set according to experimental requirements. These settings are optimized to ensure the best model performance. In practice, users can adjust parameters like batch size and training iterations based on hardware resources to optimize training. (4) Model training and evaluation: The model’s performance is evaluated using the Root Mean Square Error (RMSE), where lower RMSE values indicate better model performance. The code is provided in the appendix with no access restrictions, aiming to help readers replicate the experiments and results presented here.Experimental design and performance evaluationExperimental materialsThe dataset utilized in this research is the MovieLens dataset curated by the GroupLens organization. The MovieLens dataset is a widely used public dataset for recommender system research, available in various scales. It includes user ratings for movies, movie information, and user profiles. The attribute information used in this research encompasses user ID, age, gender, movie titles, genres, and more. The selected versions of the MovieLens dataset are 100 K and 1 M, with rating quantities of 100,000 and 1,000,209, and user counts of 943 and 6040, respectively. These datasets consist of 1682 and 3706 movies, respectively. Each dataset is divided into training, testing, and validation sets in an 8:1:1 ratio.When conducting cross-cultural research using the MovieLens dataset, even though the dataset itself does not explicitly contain cross-cultural data, appropriate data processing and experimental design can effectively explore the adaptability of recommendation systems in diverse cultural contexts. To make the MovieLens dataset suitable for cross-cultural research, this research adopts several methods to enhance the cultural relevance of the data.Firstly, user data is categorized to simulate different cultural backgrounds. Although the user information in the MovieLens dataset primarily includes age and gender, users are grouped according to these attributes to represent different cultural groups. For example, users are divided by age group or gender to create user samples that represent various cultural backgrounds in the recommendation experiments. Additionally, external data sources are integrated to enrich the cultural background information of users, such as incorporating geographical or socioeconomic data to add more cultural dimensions to the user data. Secondly, movie data is classified by cultural relevance. Although the movie information in the MovieLens dataset mainly includes genres and thematic tags, these tags can be used to classify movies into different culturally relevant categories. For instance, movies are classified according to regional or cultural characteristics, such as Western films, Asian films, etc. Based on this classification, the model simulates users’ movie preferences in different cultural contexts and conducts corresponding recommendation experiments. In constructing cross-cultural recommendation scenarios, data augmentation techniques are employed to create simulated cultural environments. For example, by mixing user and movie data from different cultural backgrounds, training and testing datasets containing various cultural characteristics are generated. This approach allows for the analysis of how the recommendation system handles and recommends movies with cultural attributes in experimental settings.To evaluate the performance of the proposed model, comparative analyses are conducted, contrasting it with existing models including GC-MC26, Biased Singular Value Decomposition (BiasedSVD)27, Singular Value Decomposition (SVD++)28, Neural CF29, Light Graph Convolution Network (LightGCN), Neural Graph Collaborative Filtering (NGCF), Graph Attention Network (GAT), and the proposed GCN-CF. Evaluation metrics employed for comparison include recall, precision, and RMSE. The calculation methodology for RMSE is articulated in Eq. (10).$$\:RMSE=\sqrt{\frac{\sum\:_{u,i}{({\widehat{y}}_{ui}-{y}_{ui})}^{2}}{N}}$$ (10) In Eq. (10), the symbol N denotes the aggregate count of test samples, and \(\:{\text{y}}_{\text{u}\text{i}}\) represents the genuine rating provided by the user. A diminished RMSE signifies superior model performance.To evaluate the impact of introducing attention mechanisms on the performance of the GCN-CF model, the following comparative experiments are conducted: Experiment Group 1: GCN-CF model with attention mechanism. Experiment Group 2: Standard GCN-CF model without attention mechanism. The experiments utilize the MovieLens 100 K and 1 M datasets, which are employed separately for training and testing the two models. Both models undergo an identical training process, including data preprocessing, training, validation, and testing phases. Consistent hyperparameter settings are used to ensure a fair comparison. The evaluation metrics include Accuracy, Recall, F1-Score, Training Time, and Inference Time.Experimental environment and parameters settingThe experimental environment was established utilizing the PyTorch 1.2.0 deep learning framework, operating on the Windows 10 platform. The hardware configuration comprised an Intel Core i7-9700 K CPU, NVIDIA RTX2080-8G GPU, and 16GB of RAM. The parameter configurations employed for the model validation experiments are delineated in Table 1. In Table 1, the hyperparameter “embedding size” refers to the dimensions of the latent vectors for users and items in the model, while the “graph convolution layers” represent the number of graph convolution layers in the model. The experiment investigates the impact of different hyperparameters on the model’s performance. The embedding dimension is set to 40, verified as the optimal value through multiple experiments, effectively balancing the model’s expressive capability and training complexity. The number of training epochs is set to 200 to ensure that the model fully learns the patterns within the data while avoiding overfitting. The regularization coefficient ranges from 10⁻⁵ to 10² to control the model’s complexity and prevent overfitting, based on the practical needs of the model and experimental results. For the learning rate, a range from 0.0001 to 0.005 is tested to identify the most suitable training speed. The dropout factor is adjusted between 0.0 and 0.8, aiming to enhance the model’s generalization ability through varying degrees of regularization. The number of graph convolution layers is set to 3, determined as the optimal value through experiments, effectively capturing complex relationships within the data. The selection of these hyperparameters aims to optimize model performance and improve training efficiency and stability.Table 1 Setting of experimental parameters for model validation.Full size tablePerformance evaluationResult analysis of GCN-CF modelFigure 5 shows the RMSE results of different models on two datasets:Figure 5Comparison of RMSE for different models.Full size imageIn Fig. 5, the collective performance of all methodologies demonstrates superior outcomes on the MovieLens 1 M dataset in comparison to the MovieLens 100 K dataset. This distinction is attributed to the augmented volume of user-movie interaction data within MovieLens 1 M, resulting in more accurate predictive outcomes. Specifically, on the MovieLens 1 M dataset, the RMSE of GCN-CF showcases a reduction of 3.1%, 2.8%, 2.6%, and 0.6% relative to BiasedSVD, SVD++, NeuralCF, and GC-MC, respectively. On the MovieLens 100 K dataset, GCN-CF’s RMSE manifests a reduction of 5.2%, 3.5%, 2.8%, and 3.2% in comparison to BiasedSVD, SVD++, NeuralCF, and GC-MC, respectively. Although LightGCN, NGCF, and GAT are currently advanced recommendation system models, their performance on the MovieLens 1 M dataset still falls short compared to the GCN-CF model. This indicates that the GCN-CF model, after incorporating a dynamic attention mechanism, can more effectively capture the complex relationships between users and movies, particularly in the context of cross-cultural recommendations.Overall, the GCN-CF model not only performs excellently with small-scale data but also demonstrates superior performance on large-scale datasets. This advantage is primarily due to the introduction of the dynamic attention mechanism, which can dynamically adjust weights based on users’ real-time behaviors and cultural backgrounds, providing more personalized and accurate recommendation results. These outcomes validate the practical application value and innovativeness of the proposed method in the field of recommendation systems.In order to visually demonstrate the higher quality and stronger expressive power of the vector representations generated by the proposed GCN-CF model, the original high-dimensional vector representations are projected onto a two-dimensional plane using Principal Component Analysis (PCA) for visual analysis. In recommendation systems, the high-dimensional feature vectors generated by the model represent the characteristics of users and items. While these high-dimensional feature vectors contain rich information, directly analyzing and visualizing this data is very challenging due to the excessive dimensions. To facilitate a more intuitive understanding and comparison of the feature representations generated by different models, the experiment employs PCA to project these high-dimensional vectors onto a two-dimensional plane. Before using PCA, it is only possible to handle and analyze these feature vectors in high-dimensional space, which is not only complex but also makes direct comparisons difficult. After PCA processing, differences between the feature representations generated by different models can be intuitively observed on the two-dimensional plane. For instance, PCA visualization can reveal whether the GCN-CF model clusters user and item vectors more closely in the two-dimensional space or whether it better distinguishes between different users and items.A comparison between the visualized results of vector representations generated by the Neural CF and GCN-CF models is shown in Figs. 6 and 7. Triangles represent user vectors, circles represent item vectors, and interactions between them are indicated by the same color for both shapes. The length of the lines connecting them represents the Euclidean distance between the two vectors. The magnitude of the Euclidean distance is inversely proportional to the Euclidean similarity; longer distances correspond to lower similarity, while shorter distances correspond to higher similarity between the two vectors.Figure 6Visualization results of vector representations generated by neural CF.Full size imageFigure 7Visualization results of vector representations generated by the GCN-CF model.Full size imageIn Fig. 6, after training with the Neural CF model, there is a noticeable clustering effect in the vector representations. Nodes of the same color cluster together, and a stronger clustering effect indicates higher quality and stronger expressive power of the vector representations generated by the model.Comparing Fig. 7 with Fig. 6, the vector representations generated by the GCN-CF model exhibit a noticeable clustering effect and demonstrate better centrality of user nodes. The Euclidean distance between user nodes and the items they interacted with is shorter, indicating higher similarity. This result indirectly confirms the higher quality and stronger expressive power of the vector representations generated in this research.Based on the visual analysis, quantitative metrics are used to evaluate the clustering effectiveness of the Neural CF and GCN-CF models. These metrics include Sum of Squared Errors (SSE), Silhouette Coefficient, and Davies-Bouldin Index, providing a numerical assessment of the clustering performance of the models. Table 2 presents the quantitative results of the two models on the MovieLens 100 K and 1 M datasets:Table 2 Clustering effect of neural CF and GCN-CF models.Full size tableIn Table 2, the GCN-CF model outperforms the Neural CF model across all three key metrics: SSE, Silhouette Coefficient, and Davies-Bouldin Index. Specifically, the SSE value of the GCN-CF model is significantly lower than that of the Neural CF model, indicating that the former exhibits a tighter distribution within clusters, effectively aggregating similar users and items to provide more accurate recommendations. Additionally, the GCN-CF model has a higher Silhouette Coefficient, suggesting better performance in distinguishing differences between different clusters. This not only demonstrates strong aggregation within similar data points but also highlights its advantages in differentiating between distinct categories of data points.Furthermore, the GCN-CF model shows lower Davies-Bouldin Index values on both the MovieLens 100 K and 1 M datasets compared to the Neural CF model, further supporting the superiority of the GCN-CF model in clustering quality. A lower index value indicates better separation between clusters and higher similarity within clusters. Overall, the results of these quantitative metrics not only reinforce the visual effects observed through PCA projection plots but also provide more objective and concrete evidence of the GCN-CF model’s ability to generate high-quality user and item vector representations and capture user behavior patterns. Particularly in handling large-scale and complex user data, the GCN-CF model demonstrates improved clustering effectiveness and recommendation performance.Comparison of different performance evaluation metrics for the GCN-CF modelFigure 8 presents the comparative analysis of recall and precision among the five models.Figure 8Comparison of accuracy and recall for each model.Full size imageIn Fig. 8, across the MovieLens 100 K and MovieLens 1 M datasets, the GCN-CF model exhibits the highest recall and precision values in comparison to other models, registering recall values of 0.324 and 0.348 and precision values of 0.145 and 0.164, respectively. These findings suggest that the GCN-CF model adeptly captures user interests and behavior patterns, yielding more precise recommendations tailored to individual user preferences. The GCN-CF model integrates conventional CF techniques with advanced feature learning facilitated by GCN, enhancing its ability to discern user interests and behavior.In order to ascertain the optimal dimension for latent factor vectors (embedding size), experiments were conducted by varying the dimensions to 10, 20, 40, and 80. These investigations were executed on both datasets to identify the most suitable latent factor vector dimension for the GCN-CF model. The outcomes of these experiments are presented in Fig. 9.Figure 9RMSE for different embedding sizes.Full size imageIn Fig. 9, an increase in the embedding size correlates with a reduction in the RMSE value of the model, indicating an enhancement in its overall performance. The model achieves optimal performance when the embedding size is set to 40, reflected by the lowest RMSE values of 0.8762 and 0.8275 for MovieLens 100 K and 1 M, respectively. However, an increase in the embedding size to 80 gives rise to discernible signs of overfitting on the MovieLens 100 K dataset.Additionally, experiments were conducted by varying the number of graph convolution layers, and the outcomes are presented in Fig. 10.Figure 10RMSE Comparison for different numbers of graph convolution layers.Full size imageIn Fig. 10, the enhancement of performance in GCN-CF is observed with the augmentation of the number of graph convolution layers. For both 2 layers and 3 layers of graph convolutions, GCN-CF surpasses the performance of a single layer, underscoring that the integration of multiple graph convolution layers enables the model to capture more intricate CF information. Nevertheless, with further expansion to 4 layers, GCN-CF exhibits indications of overfitting on the MovieLens 100 K dataset. Overfitting is typically manifested by a continuous decrease in training error while the validation error increases. As the number of convolutional layers increases, the GCN-CF model becomes increasingly complex, enabling it to learn more details and features from the training data. However, when the number of convolutional layers reaches four, the model becomes overly complex and starts to overfit the noise present in the training data, leading to poorer performance on the validation data. This situation indicates that the model has learned the training data too intricately, failing to generalize effectively to unseen data. The training process includes appropriate gradient checks and optimization measures to avoid these numerical instability issues. Therefore, this research identifies the primary cause of overfitting as the excessively high complexity of the model.Conclusively, Fig. 11 provides a comparative examination of RMSE values before and after the integration of attention mechanisms and auxiliary information in GCN-CF.Figure 11RMSE Comparison with and without Attention Mechanism and Auxiliary Information.Full size imageIn Fig. 11, the performance of GCN-CF shows improvement upon the introduction of attention mechanisms on both datasets compared to the scenario without attention mechanisms. This underscores the efficacy of attention mechanisms in more precisely modeling user interest preferences. Similarly, the performance of GCN-CF experiences enhancement following the incorporation of auxiliary information on both datasets relative to the case without such auxiliary information. This underscores that the introduction of auxiliary information in the modeling of user and item vectors results in more expressive latent vectors, consequently augmenting the recommendation effectiveness of the model.Further comparisons of the performance of the GCN-CF models with and without the attention mechanism are presented in Table 3:Table 3 Performance comparison of models with and without attention mechanism.Full size tableIn Table 3, the GCN-CF model with the attention mechanism shows higher accuracy, recall, and F1 scores on the MovieLens 100 K dataset compared to the model without the attention mechanism, indicating that the attention mechanism effectively enhances the model’s recommendation performance. On the MovieLens 1 M dataset, although the improvement is not as pronounced as in the 100 K dataset, a positive impact of the attention mechanism on performance can still be observed. In terms of training and inference time, the model with the attention mechanism experiences a slight increase compared to the one without it. This is due to the additional computational overhead introduced by the attention mechanism; however, the enhanced model performance and more accurate recommendations may offset this cost. Overall, the performance improvements of the GCN-CF model with the attention mechanism suggest that the attention mechanism offers significant advantages in capturing important features and improving recommendation outcomes. Despite the increase in computational overhead, this cost is deemed acceptable in light of the performance gains. Therefore, the GCN-CF model with the attention mechanism demonstrates greater practical application potential in recommendation systems.DiscussionThe experimental and analytical results indicate the following: (1) With the increase in embedding size, the RMSE value of the model decreases, suggesting an improvement in the model’s performance. The model achieves the optimal effect when the embedding size is 40. However, when the embedding size increases to 80, the model exhibits overfitting on MovieLens 100 K. This may be attributed to the excessively large embedding size, causing overfitting on the training set and poor performance on the test set. (2) In the cases of 2 and 3 graph convolution layers, GCN-CF outperforms the single-layer scenario, indicating that increasing the number of graph convolution layers can capture CF information. However, when the number of graph convolution layers further increases to 4, the model experiences overfitting on MovieLens 100 K, possibly because too many layers result in the model learning noise from the training set and performing poorly on the test set. (3) Introducing attention mechanisms and auxiliary information are two key factors in the model. Experimental results show that after incorporating attention mechanisms, the model performs better on both datasets compared to the case without attention mechanisms, indicating that attention mechanisms can more accurately model user interest preferences. Similarly, with the introduction of auxiliary information, the model’s performance on both datasets is superior to the scenario without auxiliary information, demonstrating that incorporating auxiliary information can generate more expressive user and item latent vectors, thereby enhancing the model’s recommendation effectiveness. Through the discussion of these parameters, a deeper understanding of their impact on model performance is gained, providing valuable guidance for the optimization and adjustment of the model.Additionally, Nassar et al. (2020) proposed a novel deep learning-based multi-criteria CF model, amalgamating multi-criteria recommendation, CF, and deep learning. Experimental outcomes demonstrated its superiority over other state-of-the-art methods on real-world datasets30. In a comprehensive review of recent research endeavors in GNN-based recommendation systems, Wu et al. (2022) provided a classification based on the types of information and recommendation tasks, categorizing GNN-based recommendation models. Their research systematically analyzed challenges in applying GNN to different data types and discussed existing solutions in the field31. Safavi and Jalali (2022) introduced a unique approach to interest point recommendation employing deep learning and CNNs. This approach considered the impact of the most similar friendship patterns for recommendations rather than accounting for all user-to-user friendship relations, and results indicated that analyzing similar friendship relationships can lead to more accurate recommendations32. Overall, the growing body of research suggests that the application of deep learning methods, such as convolutional networks, to recommendation algorithms can enhance the accuracy of recommendation systems.Drawing from the above analysis, the following summarization of cross-cultural dissemination strategies for animated films is presented: The primary strategy involves a nuanced understanding of differences between various cultures and the subsequent adjustment to diverse cultural audiences’ psychological, valuational, and aesthetic preferences. Modifications to the film’s plot, characters, dialogues, etc., employ a cultural lens to ensure its appeal and resonance in diverse cultural contexts. Tailored promotional strategies and market positioning are adopted in each target culture to align with localized preferences. Localized promotional plans are devised to attract local audiences by discerning cultural characteristics and audience preferences in each region. When disseminating to different cultures, appropriate local dubbing and subtitling methods are chosen to ensure the accurate conveyance of dialogues and emotions, thus enhancing emotional resonance. The integration of IoT technologies provides opportunities for audience participation and interaction, including applications of virtual reality, augmented reality, and similar technologies, thereby enhancing audience engagement.In addition, Liu et al. (2023) proposed a novel privacy-preserving reputation update scheme for cloud-assisted vehicular networks. In this scheme, reputation feedback is collected and pre-processed by honest yet untrusted entities, while a curious cloud service provider participates in a privacy-preserving manner. By introducing the cloud service provider, the scheme successfully reduces the computation and communication overhead of the authority entity, decreasing them by approximately 88.36% and 83.88%, respectively. Simultaneously, the scheme is still able to provide robust privacy protection, high security, and acceptable computation and communication overhead33. This research outcome offers new insights and solutions for enhancing vehicular network performance, reducing management costs, and maintaining user privacy. Guo et al. (2023) proposed a trust assessment scheme for federated learning in digital twin networks, considering both direct trust evidence and recommended trust information. This scheme finely describes user behavior through a multi-attribute user behavior model. Experimental results demonstrate that the scheme accurately assesses users’ trust levels with different behavior patterns and effectively counters attacks where users alternate between good and malicious behavior34. The scheme excels in resisting attacks involving users alternating between good and malicious behavior. This research provides an effective approach for building trust in federated learning in digital twin networks, contributing to ensuring user reliability and the security of federated learning. Integrating the studies of these two scholars, it is evident that they offer innovative privacy-preserving and trust management solutions in different domains (vehicular networks and digital twin networks). Both emphasize the introduction of new technological means, such as cloud services and federated learning, to address issues present in traditional solutions. Meanwhile, this research explores the possibility of applying the IoT and convolutional networks to the dissemination of animated films in a cross-cultural context and constructs a CF movie recommendation model based on GCN. Through experiments, it has been demonstrated to exhibit significant superiority on different datasets, showing higher detection accuracy and performance. The integration of these research outcomes is expected to provide comprehensive and robust support for the future development of intelligent transportation systems, digital twin networks, and the field of cultural exchange.This research, through integrating the IoT and GCN, has deeply influenced film production, dissemination, cultural exchange, and the global media industry. This approach enables filmmakers to gain deeper insights into audience feedback and preferences. By collecting data through IoT sensors, film producers can access real-time interactions and emotional changes during the viewing process, providing more references for film production. It forms the foundation for personalized content creation, catering better to audience interests and demands. Considering cultural differences, societal environments, and audience preferences is crucial in cross-cultural communication. The fusion of IoT and GCN offers more accurate recommendations for global communication and allows adjustments based on audience feedback from diverse cultural backgrounds, implementing adaptive communication strategies. The theory of cross-cultural communication emphasizes cultural differences in information transmission and meaning construction. The integration of IoT and GCN surpasses cultural barriers through personalized recommendations, fostering more similar emotional experiences among audiences from different cultural backgrounds. This promotes cultural exchange and understanding. The proposed integration strategy brings innovative applications to the global media industry. Through interactive experiences, personalized recommendations, and innovative applications of CF, it profoundly impacts the audience’s viewing experience, injecting new vitality into the global media industry.This approach enhances the connection between the audience and content by providing real-time audience feedback and a dynamic adjustment mechanism, improving audience satisfaction and driving the global media industry toward more innovative and personalized directions. In summary, this research introduces new ideas and methods that bring fresh perspectives to the industry. It not only fosters innovative development in the global media sector but also offers effective solutions for cross-cultural communication. This innovative integration approach provides more precise and personalized support for film production and recommendation systems, demonstrating the immense potential of the IoT and GCN in the modern media industry.ConclusionResearch contributionIn order to investigate the utilization of the IoT and convolutional networks in the cross-cultural dissemination of animated films, this research constructs a CF movie recommendation model based on GCN. The experimental findings yield the following conclusions: (1) The RMSE of the GCN-CF model on the MovieLens 100 K and 1 M datasets is 0.8762 and 0.8275, respectively. Comparative analysis with other models reveals a reduction ranging from 0.6 to 5.2%, signifying superior detection accuracy and enhanced performance of the GCN-CF model. Additionally, the GCN-CF model exhibits the highest recall and precision in comparison to other models, with recall values of 0.324 and 0.348 and precision values of 0.145 and 0.164, respectively. These findings indicate that the GCN-CF model adeptly captures user interests and behavior patterns, providing more focused recommendations aligned with user preferences. (2) The introduction of attention mechanisms and auxiliary information enhances model performance on both datasets, resulting in reductions in RMSE ranging from 0.4%. This underscores the efficacy of attention mechanisms and auxiliary information in improving the model’s recommendation accuracy. (3) Strategies for disseminating cross-cultural animated films should holistically consider cultural disparities, social contexts, and audience preferences. Diverse methodologies and approaches must be employed to achieve successful film dissemination across various cultures. Future works and research limitationsWhile the GCN-CF model demonstrates outstanding performance in the conducted experiments, its applicability may be constrained by different film genres and target audience demographics. Subsequent research should investigate the model’s efficacy in varied scenarios and delve deeper into understanding cultural discrepancies, encompassing emotional experiences, values, etc. This will facilitate the development of more nuanced strategies tailored to the diverse needs of cultural audiences. Additionally, the integration of IoT in film dissemination can be further expanded to foster interactive engagements with the audience, delivering personalized recommendations and enhancing the overall viewing experience. Data availability The datasets used and/or analysed during the current study available from the corresponding author Haibin Xia on reasonable request via e-mail bankeyxia@gmail.com. ReferencesWardaniningsih, A. D. & Kasih, E. N. E. W. Delineation of women identity in the Disney animated film Encanto (2019). Lire J 6(2), 209–229 (2022). Google Scholar Wen, R. Subtile translation of Chinese animated film from the perspective of adaptation theory: Nezha, I am the destiny as an example. Open Access Library J. 9(7), 1–9 (2022). Google Scholar Wu, B. et al. EAGCN: An efficient adaptive graph convolutional network for item recommendation in social Internet of Things. IEEE Internet Things J. 9(17), 16386–16401 (2022).Article Google Scholar Nauman, A. et al. Multimedia Internet of Things: A comprehensive survey. IEEE Access 8, 8202–8250 (2020).Article Google Scholar Chaudhuri, R. et al. Antecedents to enculturation and acculturation for diffusion of knowledge using internet applications: An empirical investigation. J. Glob. Bus. Adv. 15(3), 369–391 (2022). Google Scholar Xie, D. & Yin, C. Exploration of Chinese cultural communication mode based on the Internet of Things and mobile multimedia technology. PeerJ Comput. Sci. 9, e1330 (2023).Article PubMed PubMed Central Google Scholar Zhang, Q., Lu, J. & Jin, Y. Artificial intelligence in recommender systems. Complex Intell. Syst. 7, 439–457 (2021).Article Google Scholar Da’u, A. & Salim, N. Recommendation system based on deep learning methods: A systematic review and new directions. Artif. Intell. Rev. 53(4), 2709–2748 (2020).Article Google Scholar Singh, J. An efficient deep neural network model for music classification. Int. J. Web Sci. 3 (3), 236–248 (2022).Article Google Scholar Özçelik, Y. B. & Altan, A. Overcoming nonlinear dynamics in diabetic retinopathy classification: A robust AI-based model with chaotic swarm intelligence optimization and recurrent long short-term memory. Fractal Fract. 7(8), 598 (2023).Article Google Scholar Karasu, S. & Altan, A. Agricultural crop classification with R-CNN and machine learning methods. Int. Mediterr. Congr. 28, 101–123 (2022). Yağ, I. & Altan, A. Artificial intelligence-based robust hybrid algorithm design and implementation for real-time detection of plant diseases in agricultural environments. Biology 11(12), 1732 (2022).Article PubMed PubMed Central Google Scholar Wang, S. et al. A survey on session-based recommender systems. ACM Comput. Surv. 54(7), 1–38 (2021).Article MathSciNet Google Scholar Wu, L. et al. A survey on accuracy-oriented neural recommendation: From collaborative filtering to information-rich recommendation. IEEE Trans. Knowl. Data Eng. 35(5), 4425–4445 (2022).ADS Google Scholar Wu, X. et al. Inferring LncRNA-disease associations based on graph autoencoder matrix completion. Comput. Biol. Chem. 87, 107282 (2020).Article CAS PubMed Google Scholar Li, Z. et al. Learning knowledge graph embedding with heterogeneous relation attention networks. IEEE Trans. Neural Netw. Learn. Syst. 33(8), 3961–3973 (2021).Article MathSciNet Google Scholar Tang, H. et al. Dynamic evolution of multi-graph based collaborative filtering for recommendation systems. Knowl. Based Syst. 228, 107251 (2021).Article Google Scholar Chen, M. S. et al. Representation learning in multi-view clustering: A literature review. Data Sci. Eng. 7(3), 225–241 (2022).Article Google Scholar Dash, B. & Sharma, P. Role of artificial intelligence in smart cities for information gathering and dissemination (a review). Acad. J. Res. Sci. Publish. 4(39), 187–198 (2022).Wei, X. et al. Online social network information dissemination integrating overconfidence and evolutionary game theory. IEEE Access 9, 90061–90074 (2021).Article Google Scholar Davis, R. & D’Lima, D. Building capacity in dissemination and implementation science: A systematic review of the academic literature on teaching and training initiatives. Implement. Sci. 15, 1–26 (2020).Article Google Scholar Chung, H. et al. Brick-by-brick: combinatorial construction with deep reinforcement learning. Adv. Neural Inf. Process. Syst. 34, 5745–5757 (2021).MathSciNet Google Scholar Song, C. et al. Flexible, graphene-based films with three-dimensional conductive network via simple drop-casting toward electromagnetic interference shielding. Compos. Commun. 24, 100632 (2021).Article Google Scholar Rees-Roberts, N. After fashion film: Social video and brand content in the influencer economy. J. Vis. Cult. 19(3), 405–421 (2020).Article Google Scholar Salim, S. et al. Perturbation-enabled deep federated learning for preserving Internet of Things-based social networks. ACM Trans. Multimed. Comput. Commun. Appl. 18(2s), 1–19 (2022).Article Google Scholar Jin, C. et al. Predicting miRNA-disease association based on neural inductive matrix completion with graph autoencoders and self-attention mechanism. Biomolecules 12(1), 64 (2022).Article CAS PubMed PubMed Central Google Scholar He, M. et al. Long short-term memory network with multi-resolution singular value decomposition for prediction of bearing performance degradation. Measurement 156, 107582 (2020).Article Google Scholar Herviou, L., Bardarson, J. H. & Regnault, N. Defining a bulk-edge correspondence for non-Hermitian Hamiltonians via singular-value decomposition. Phys. Rev. A 99(5), 052118 (2019).Article ADS CAS Google Scholar Do, P. M. T. & Nguyen, T. T. S. Semantic-enhanced neural collaborative filtering models in recommender systems. Knowl. Based Syst. 257, 109934 (2022).Article Google Scholar Nassar, N., Jafar, A. & Rahhal, Y. A novel deep multi-criteria collaborative filtering model for recommendation system. Knowl. Based Syst. 187, 104811 (2020).Article Google Scholar Wu, S. et al. Graph neural networks in recommender systems: A survey. ACM Comput. Surv. 55(5), 1–37 (2022).Article Google Scholar Safavi, S. & Jalali, M. DeePOF: A hybrid approach of deep convolutional neural network and friendship to point-of-interest recommendation system in location-based social networks. Concurr Comput. Pract. Exp. 34(15), e6981 (2022).Article Google Scholar Liu, Z. et al. PPRU: A Privacy-Preserving Reputation Updating Scheme for Cloud-Assisted Vehicular Networks. IEEE Trans. Veh. Technol. 34(5), 101–131 (2023).Guo, J. et al. TFL-DT: A trust evaluation scheme for federated learning in digital twin for mobile networks. IEEE J. Sel. Areas Commun. 41(11), 123–145 (2023).Download referencesAuthor informationAuthors and AffiliationsFaculty of Information Technology, Concord University College Fujian Normal University, Fuzhou, ChinaSheng YeCollege of Art and Design, Guangdong Eco-Engineering Polytechnic, Guangzhou, ChinaQian HuangDigital Media Art Department of College of Arts, East China Jiaotong University, Nanchang, 330013, ChinaHaibin XiaAuthorsSheng YeView author publicationsYou can also search for this author in PubMed Google ScholarQian HuangView author publicationsYou can also search for this author in PubMed Google ScholarHaibin XiaView author publicationsYou can also search for this author in PubMed Google ScholarContributionsSheng Ye: Conceptualization, methodology, software, validation, formal analysis, investigation, resources, data curation, writing—original draft preparationQian Huang: methodology, software, validation, formal analysis, investigation, resources, data curationHaibin Xia: writing—review and editing, visualization, supervision, project administration, funding acquisition.Corresponding authorCorrespondence to Haibin Xia.Ethics declarations Competing interests The authors declare no competing interests. Additional informationPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Supplementary InformationSupplementary Material 1.Rights and permissions Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissionsAbout this articleCite this articleYe, S., Huang, Q. & Xia, H. Personalized movie recommendation in IoT-enhanced systems using graph convolutional network and multi-layer perceptron. Sci Rep 14, 25268 (2024). https://doi.org/10.1038/s41598-024-76587-4Download citationReceived: 02 April 2024Accepted: 15 October 2024Published: 25 October 2024DOI: https://doi.org/10.1038/s41598-024-76587-4Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative KeywordsAnimation filmsCross-cultural communicationGraph convolutional neural networkCollaborative filteringCommunication strategy Download PDF Advertisement Search Search articles by subject, keyword or author Show results from All journals This journal Search Advanced search Quick links Explore articles by subject Find a job Guide to authors Editorial policies Close banner Close Sign up for the Nature Briefing: AI and Robotics newsletter — what matters in AI and robotics research, free to your inbox weekly. Email address Sign up I agree my information will be processed in accordance with the Nature and Springer Nature Limited Privacy Policy. Close banner Close Get the most important science stories of the day, free in your inbox. Sign up for Nature Briefing: AI and Robotics